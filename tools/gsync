#!/usr/bin/env python3
# Ehi, Emacs this is -*- python -*-

from rich.console import Console
from rich.logging import RichHandler
from rich.pretty import pprint
from rich.rule import Rule
from rich.traceback import install as install_rich_traceback
from textwrap import dedent
import atexit
import click
import getpass
import git
import logging
import os
import paramiko
import queue
import re
import select
import socket
import sys
import threading
import time
import uuid
import xdg
import yaml

# example of ~/.config/gsync/settings.yaml
# common:
#   tolerate_uncommitted: yes
#   tolerate_untracked: ask

# vdesk:
#   host: "vdesk"
#   repo: "/home/mvitale/uno_sync"

install_rich_traceback(show_locals=False, suppress=[click])

console = Console()


def configure_logging(log_level):
   global logger
   numeric_level = getattr(logging, log_level.upper(), None)
   if not isinstance(numeric_level, int):
      raise ValueError(f"Invalid log level: {log_level}")
   logging.basicConfig(level=numeric_level, format='%(message)s', handlers=[RichHandler()])

def timed(func):
    def wrap_func(*args, **kwargs):
        t1 = time.time()
        result = func(*args, **kwargs)
        t2 = time.time()
        logging.info(f'Function {func.__name__!r} executed in {(t2-t1):.4f}s')
        return result
    return wrap_func

def get_workspace():
   current_dir = os.getcwd()
   current_dir = os.path.abspath(current_dir)
   home_dir = os.path.expanduser("~")
   root_dir = os.path.abspath(os.sep)

   while True:
      filepaths = [os.path.join(current_dir, filename) for filename in ["WORKSPACE", "WORKSPACE.bazel"]]
      for f in filepaths:
         if os.path.exists(f):
            return current_dir

      if current_dir == home_dir or current_dir == root_dir:
            return None

      current_dir = os.path.abspath(os.path.join(current_dir, os.pardir))

def get_repo():
   top_dir = get_workspace()
   return git.Repo(top_dir)

def get_context(args):
   config_dir = os.path.join(xdg.BaseDirectory.xdg_config_home, "gsync")
   config_file = os.path.join(config_dir, "settings.yaml")

   with open(config_file,'r') as f:
      output = yaml.safe_load(f)

   context = {**output.get("common", {}), **output.get(args["remote"], {}), **args}

   conn = paramiko.SSHClient()

   conn.set_missing_host_key_policy(paramiko.AutoAddPolicy())
   try:
      username = getpass.getuser()
      home_directory = os.path.expanduser('~')
      # use identity from config instead of vdesk.pub
      conn.connect(context["host"], username=username, key_filename=f"${home_directory}/.ssh/vdesk.pub")
      atexit.register(lambda: conn.close())
   except (paramiko.SSHException, socket.error):
      logging.fatal(f"Cannot connect to {context['host']}")
      sys.exit(1)

   context["ssh"] = conn
   context["local_repo"] = get_repo()
   return context

def read_stream(stream, stream_name, out_queue):
    while True:
        line = stream.readline()
        if line:
           out_queue.put((stream_name, line.strip()))
        else:
            break

def print_output_main(out_queue, print_lock, print_output):
   while True:
      stream_name, line = out_queue.get()
      if line is None:
         break
      if print_output:
         with print_lock:
            print(f"{line}")

def ssh_exec(context, command=None, print_output=False):
   if command == None:
      command = " ".join(context["command"])
   command = f"cd {context['repo']}; {command}"

   try:
      ssh = context["ssh"]
      transport = ssh.get_transport()
      channel = transport.open_session()

      stdin, stdout, stderr = ssh.exec_command(command)

      out_queue = queue.Queue()
      print_lock = threading.Lock()

      stdout_thread = threading.Thread(target=read_stream, args=(stdout, "STDOUT", out_queue),)
      stderr_thread = threading.Thread(target=read_stream, args=(stderr, "STDERR", out_queue),)
      print_thread = threading.Thread(target=print_output_main, args=(out_queue, print_lock, print_output),)

      stdout_thread.start()
      stderr_thread.start()
      print_thread.start()

      stdout_thread.join()
      stderr_thread.join()
      out_queue.put(("NONE", None))
      print_thread.join()

   except paramiko.SSHException as e:
        logging.error(f"SSHException: {str(e)}")
   except Exception as e:
        logging.error(f"Exception: {str(e)}")


def ensure_remote_repo(context):
   repo_path = context["repo"]
   try:
        ssh = context["ssh"]
        stdin, stdout, stderr = ssh.exec_command(f"ls {repo_path}")
        if stderr.read():
            logging.info("Repository does not exist, creating now...")
            stdin, stdout, stderr = ssh.exec_command(
                f"mkdir -p {repo_path} && cd {repo_path} && git init && git commit -m 'first' --allow-empty"
            )
            if stderr.read():
                logging.error("Error creating repository")
            else:
                logging.info("Repository created successfully")
            push_master(context)
        else:
            logging.info("Repository exists")

   except Exception as e:
      logging.critical(f"An error occurred: {str(e)}")
      sys.exit(1)


def ensure_remote(context):
   remote = context["remote"]
   remotes = context["local_repo"].remotes
   # here we should check that the URL hasn't changes. If so we need to remove and add
   # back a new remote specification
   if remote in [r.name for r in remotes]:
      return
   remote_url=f"ssh://{context.get('host', remote)}:{context['repo']}"
   try:
      new_remote = context["local_repo"].create_remote(remote, url=remote_url)
   except git.GitCommandError as e:
      raise git.GitCommandError(f"Failed to add remote: {str(e)}")

def get_stash_index(git, stash_uuid):
   pattern = re.compile(r'stash@\{(?P<stash_id>\d+)\}:.*?' + re.escape(stash_uuid))

   for stash in git.stash('list').splitlines():
      match = pattern.search(stash)
      if match:
        stash_id = match.group('stash_id')
        return stash_id

def push_master(context):
   repo = context["local_repo"]

   remote = repo.remote(name=context["remote"])
   try:
      # master shoudln't be hardcoded. Same for main. And they shopuld probably be the same
      push_info = remote.push(refspec='main:master', force=True)
   except git.GitCommandError as error:
      logging.error(f"An error occurred: {str(error)}")

def push_local_state(context):
   repo = context["local_repo"]

   remote = repo.remote(name=context["remote"])
   active_branch = repo.active_branch

   # 'master' shouldn't be hardcoded
   if active_branch.name == 'master':
      # would require to have a different branch to switch to
      logging.error("Cannot run gsync from master")
      sys.exit(1)

   sync_uuid = str(uuid.uuid4())

   git = repo.git
   git.stash("save", "--keep-index", sync_uuid)
   # typically '0'
   stash_index = get_stash_index(git, sync_uuid)

   logging.error("TODO: switch to whatever the default branch is on the remote")
   ssh_exec(context, "git checkout master")

   new_branch_name = sync_uuid
   new_branch = repo.create_head(new_branch_name)
   new_branch.checkout()

   if stash_index is not None:
      git.stash("apply", stash_index)
      git.add("--all")
      git.commit("-m", "gsync")

   try:
      push_info = remote.push(refspec=f'{new_branch_name}:{active_branch.name}', force=True)
   except git.GitCommandError as error:
      logging.error(f"An error occurred: {str(error)}")

   ssh_exec(context, f"git checkout {active_branch.name}")

   active_branch.checkout()
   if stash_index is not None:
      git.stash("pop", "--index", stash_index)
   repo.delete_head(new_branch, force=True)

@timed
def sync(context):
   ensure_remote(context)
   ensure_remote_repo(context)
   ensure_remote_script(context)
   push_local_state(context)


def is_safe_sandbox(context):
   repo = context["local_repo"]

   # not in the middle of a merge, rebase, cherry picking, bisecting or any other wizardry
   git_dir = repo.git_dir
   for tombstone in ["MERGE_HEAD", "REBASE_APPLY", "REBASE_MERGE", "REBASE-i", "CHERRY_PICK_HEAD", "BISECT_LOG"]:
      if os.path.exists(os.path.join(git_dir, tombstone)):
         return False

   # no unacceptable untracked files.
   blocking_untracked_files = []
   nonblocking_untracked_files = []
   for f in repo.untracked_files:
      absolute_f = os.path.abspath(f)
      if os.path.dirname(absolute_f) == os.path.dirname(git_dir):
         nonblocking_untracked_files.append(f)
      else:
         blocking_untracked_files.append(f)

   # for f in blocking_untracked_files:
   #    print(f"--> {f}")

   tolerate_uncommitted = context.get("tolerate_uncommitted", False)
   if repo.is_dirty():
      logging.info("You have uncommitted changes:")

      diff = repo.index.diff(None)

      if not tolerate_uncommitted and diff:
         for file_diff in diff:
            logging.info(f"{file_diff.change_type} file: {file_diff.b_path}")
         return False
   else:
      logging.info("No uncommitted changes found.")

   tolerate_untracked = context.get("tolerate_untracked", False)
   if not tolerate_untracked and  blocking_untracked_files:
      return False

   return True

@timed
def ensure_remote_script(context):
   script = '''
#!/bin/bash

LOCKFILE="/tmp/mylock"
PIDFILE="/tmp/mypid"

if [ "$1" == "force" ]; then
    if [ -e ${PIDFILE} ]; then
        kill $(cat ${PIDFILE})
        rm -f ${LOCKFILE} ${PIDFILE}
        echo "Previous process killed."
    else
        echo "No previous process found."
    fi
    exit
fi

if [ -e ${LOCKFILE} ]; then
    if [ -e ${PIDFILE} ] && kill -0 $(cat ${PIDFILE}); then
        echo "Command is already running as process $(cat ${PIDFILE})."
        exit 1
    else
        # Cleanup leftover lockfile if process not actually running
        echo "Removing stale lock file."
        rm -f ${LOCKFILE}
    fi
fi

# Ensure lockfile is removed on script exit and use the lock
trap "rm -f ${LOCKFILE} ${PIDFILE}" EXIT
touch ${LOCKFILE}
echo $$ > ${PIDFILE}

# Your command here
sleep 60

# Cleanup
rm -f ${LOCKFILE} ${PIDFILE}
'''
   wrapper_filename = "/home/mvitale/gsync_bazel_wrapper"
   sftp = context['ssh'].open_sftp()
   with sftp.file(wrapper_filename, "w") as remote_file:
      remote_file.write(dedent(script).strip('\n'))
      sftp.chmod(wrapper_filename, 0o755)
   sftp.close()

def is_port_open(host, port=22, timeout=5):
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.settimeout(timeout)
    try:
        sock.connect((host, port))
        return True
    except socket.error as err:
        logging.error(f"Connection to {host}:{port} failed: {str(err)}")
        return False
    finally:
        sock.close()

def ensure_vdesk(context):
   if not is_port_open(context["host"]):
      sys.exit(1)

@timed
def sync_and_execute(context):
   ensure_vdesk(context)
   if not is_safe_sandbox(context):
      logging.critical("Sandbox in dirty state")
      sys.exit(1)
   sync(context)
   console.print(Rule(characters="─"))
   ssh_exec(context, print_output=True)

@click.command()
@click.option('--remote', default="vdesk", help='The remote we sync to and execute commands on')
@click.option("--log", default="error", help='Debug level: debug, info, warning, error, critical')
@click.argument('command', nargs=-1)
def main(remote, command, log):
   args = {
      "command": command,
      "remote": remote,
   }

   configure_logging(log)

   context = get_context(args)
   pprint(context)
   sync_and_execute(context)

if __name__ == '__main__':
   main()
